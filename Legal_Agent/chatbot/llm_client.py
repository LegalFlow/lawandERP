from openai import OpenAI
from typing import List, Dict, Any
import anthropic  # Claude APIë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import requests.adapters
import urllib3.util.timeout

# ì„¤ì • ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
import config

# íƒ€ì„ì•„ì›ƒ ì„¤ì • ë³€ìˆ˜ ì •ì˜ (ì´ˆ ë‹¨ìœ„)
REQUEST_TIMEOUT = config.DEFAULT_TIMEOUT  # config.pyì—ì„œ ì •ì˜í•œ íƒ€ì„ì•„ì›ƒ ì‚¬ìš©

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=config.OPENAI_API_KEY, timeout=REQUEST_TIMEOUT)

# Claude í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
claude_client = anthropic.Anthropic(api_key=config.CLAUDE_API_KEY, timeout=REQUEST_TIMEOUT)

def format_articles_for_context(articles: List[Dict[str, Any]]) -> str:
    """
    ê²€ìƒ‰ëœ ë²•ë¥  ì¡°ë¬¸ì„ LLM í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  í˜•ì‹ìœ¼ë¡œ ê°€ê³µí•©ë‹ˆë‹¤.
    
    Args:
        articles: ê²€ìƒ‰ëœ ë²•ë¥  ì¡°ë¬¸ ëª©ë¡
        
    Returns:
        í¬ë§·íŒ…ëœ ì¡°ë¬¸ ë¬¸ìì—´
    """
    if not articles:
        return "ê´€ë ¨ ë²•ë¥  ì¡°ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    formatted_text = "### ê´€ë ¨ ë²•ë¥  ì¡°ë¬¸\n\n"
    
    for i, article in enumerate(articles, 1):
        formatted_text += f"[{i}] {article['law_name']} ì œ{article['article_no']}ì¡°"
        
        if article['article_title']:
            formatted_text += f" ({article['article_title']})"
        
        formatted_text += "\n"
        
        # ê³„ì¸µ êµ¬ì¡° ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        hierarchy = []
        if article.get('part_title'):
            hierarchy.append(f"[í¸] {article['part_title']}")
        if article.get('chapter_title'):
            hierarchy.append(f"[ì¥] {article['chapter_title']}")
        if article.get('section_title'):
            hierarchy.append(f"[ì ˆ] {article['section_title']}")
            
        if hierarchy:
            formatted_text += f"ìœ„ì¹˜: {' > '.join(hierarchy)}\n"
        
        # ì¡°ë¬¸ ë‚´ìš©
        formatted_text += f"ë‚´ìš©: {article['content']}\n\n"
    
    return formatted_text

def generate_response(question: str, articles: List[Dict[str, Any]], history: List[Dict] = None, model: str = None) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ ì¡°ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        articles: ê´€ë ¨ ë²•ë¥  ì¡°ë¬¸ ëª©ë¡
        history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒì‚¬í•­)
        model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: configì— ì§€ì •ëœ ëª¨ë¸)
        
    Returns:
        ìƒì„±ëœ ë‹µë³€
    """
    # ê´€ë ¨ ì¡°ë¬¸ í¬ë§·íŒ…
    context = format_articles_for_context(articles)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """
    ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì±„ë¬´ì íšŒìƒ ë° íŒŒì‚°ì— ê´€í•œ ë²•ë¥  ì „ë¬¸ AI ë¹„ì„œì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë²•ë¥  ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°ê´€ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

    ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:
    0. ì±„ë¬´ì íšŒìƒ ë° íŒŒì‚°ì— ê´€í•œ ë²•ë¥ ì„ ìµœìš°ì„ ìœ¼ë¡œ ê²€í† í•˜ê³ , íšŒìƒìœ„ì› ì‹¤ë¬´í¸ëŒì´ë‚˜ íŒŒì‚°ê´€ì¬ì¸ ì§ë¬´í¸ëŒì„ ì°¸ê³ í•œ í›„ ë¯¼ë²• ë“± ê¸°íƒ€ ë²•ë¥ ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
    1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°ì—” ë¨¼ì € ì§ˆë¬¸í•œ ë’¤, ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”.
    2. ë²•ë¥  ì¡°ë¬¸ì˜ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•  ë•ŒëŠ” ëª…í™•íˆ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”. (ì˜ˆ: ì±„ë¬´ì íšŒìƒ ë° íŒŒì‚°ì— ê´€í•œ ë²•ë¥  ì œ580ì¡°ì— ë”°ë¥´ë©´...)
    3. ë²•ë¥  ì¡°ë¬¸ì„ ê·¼ê±°ë¡œ ì´ì•¼ê¸°ë¥¼ í•˜ë©´ í•´ë‹¹ ë²•ë¥  ì¡°ë¬¸ì„ ë°˜ë“œì‹œ í‘œì‹œí•´ ì£¼ì„¸ìš”.
    4. ì‘ë‹µì€ êµ¬ì¡°í™”ë˜ê³  ì‹œê°ì ìœ¼ë¡œ ëª…í™•í•˜ê²Œ êµ¬ì„±í•˜ì„¸ìš”.
    5. ë‚´ìš©ì„ ë‹¨ê³„ë³„ë¡œ êµ¬ë¶„í•˜ê³ , ê° ì„¹ì…˜ì— ì ì ˆí•œ ì œëª©ì„ ì‚¬ìš©í•˜ì„¸ìš”. 
    6. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš” (âœ…, ğŸ”¹, âš–ï¸, ğŸ“ ë“±).
    7. ë³µì¡í•œ ì •ë³´ëŠ” í‘œ í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.
    8. ì¤‘ìš”í•œ ì •ë³´ëŠ” êµµì€ ê¸€ì”¨ë‚˜ í‘œì‹œë¥¼ í†µí•´ ê°•ì¡°í•˜ì„¸ìš”.
    9. ë¹„êµê°€ í•„ìš”í•œ ë‚´ìš©ì€ ë¹„êµí‘œë¥¼ ë§Œë“¤ì–´ ëª…í™•íˆ ì°¨ì´ì ì„ ë³´ì—¬ì£¼ì„¸ìš”.

    ë‹µë³€ í˜•ì‹:
    - ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ë²•ë¥  ì „ë¬¸ê°€ì²˜ëŸ¼ ì‘ë‹µí•˜ì„¸ìš”.
    - ë¨¼ì € ê°„ê²°í•œ ìš”ì•½ ë‹µë³€ì„ ì œì‹œí•œ í›„ ì„¸ë¶€ ë‚´ìš©ì„ êµ¬ì¡°í™”í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.
    - ë§ˆì§€ë§‰ì—ëŠ” ì‹¤ë¬´ì  í•¨ì˜ë‚˜ ì¶”ê°€ ì¡°ì–¸ì„ ë§ë¶™ì´ì„¸ìš”.
    - í•„ìš”í•˜ë‹¤ë©´ ì‚¬ë¡€ë‚˜ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ì„¸ìš”.
    """
    
    # ëª¨ë¸ ì„¤ì •
    use_model = model or config.LLM_MODEL
    
    # ëª¨ë¸ì´ claudeë¡œ ì‹œì‘í•˜ë©´ Claude API ì‚¬ìš©, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ OpenAI API ì‚¬ìš©
    if use_model.startswith("claude"):
        try:
            # ìµœì‹  ë²„ì „ API ì‹œë„ (messages.create)
            messages = [
                {"role": "user", "content": f"ì§ˆë¬¸: {question}\n\n{context}"}
            ]
            
            # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
            if history:
                formatted_history = []
                for msg in history:
                    formatted_history.append({"role": msg["role"], "content": msg["content"]})
                
                # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ contextì™€ í•¨ê»˜ ì—…ë°ì´íŠ¸
                if formatted_history and formatted_history[-1]["role"] == "user":
                    formatted_history.pop()  # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì œê±°
                    
                messages = formatted_history + messages
            
            # ìµœì‹  ë²„ì „ API í˜¸ì¶œ - íƒ€ì„ì•„ì›ƒ ì ìš©
            response = claude_client.messages.create(
                model=use_model,
                system=system_prompt,
                messages=messages,
                max_tokens=15000,
                temperature=0.2,
                timeout=REQUEST_TIMEOUT,  # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
            )
            
            return response.content[0].text
        except (AttributeError, TypeError) as e:
            print(f"ìµœì‹  Claude API í˜¸ì¶œ ì‹¤íŒ¨, ì´ì „ ë²„ì „ API ì‹œë„: {e}")
            try:
                # ì´ì „ ë²„ì „ API ì‹œë„ (completions.create)
                prompt = f"\n\nHuman: {question}\n\n{context}\n\nAssistant:"
                
                # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                if history:
                    conversation_history = ""
                    for msg in history:
                        role_prefix = "Human: " if msg["role"] == "user" else "Assistant: "
                        conversation_history += f"\n\n{role_prefix}{msg['content']}"
                    
                    prompt = f"{conversation_history}\n\nHuman: {question}\n\n{context}\n\nAssistant:"
                
                # ì´ì „ ë²„ì „ API í˜¸ì¶œ - íƒ€ì„ì•„ì›ƒ ì ìš©
                response = claude_client.completions.create(
                    prompt=f"{system_prompt}\n{prompt}",
                    model=use_model,
                    max_tokens_to_sample=15000,
                    temperature=0.2,
                    stop_sequences=["\n\nHuman:"],
                    timeout=REQUEST_TIMEOUT,  # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
                )
                
                return response.completion
            except Exception as e2:
                print(f"ì´ì „ Claude APIë„ ì‹¤íŒ¨, OpenAIë¡œ í´ë°±: {e2}")
                use_model = config.GPT_MODEL
                # ì•„ë˜ì˜ OpenAI ì½”ë“œë¡œ ê³„ì† ì§„í–‰
    
    # OpenAI API ì‚¬ìš©
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {"role": "system", "content": system_prompt},
    ]
    
    # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
    if history:
        for msg in history:
            messages.append({"role": msg["role"], "content": msg["content"]})
    
    # í˜„ì¬ ì§ˆë¬¸ ë° ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    messages.append({"role": "user", "content": f"ì§ˆë¬¸: {question}\n\n{context}"})
    
    # ë‹µë³€ ìƒì„± - íƒ€ì„ì•„ì›ƒ ì ìš© (í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œ ì´ë¯¸ ì„¤ì •ë¨)
    response = client.chat.completions.create(
        model=use_model,
        messages=messages,
        temperature=0.2,
        max_tokens=15000,
    )
    
    return response.choices[0].message.content

def generate_general_response(question: str, history: List[Dict] = None, model: str = None) -> str:
    """
    GENERAL ëª¨ë“œ: RAG ì—†ì´ ì§ì ‘ LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒì‚¬í•­)
        model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: configì— ì§€ì •ëœ ëª¨ë¸)
        
    Returns:
        ìƒì„±ëœ ë‹µë³€
    """
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """
    ë‹¹ì‹ ì€ 'ê°œì¸íšŒìƒ ë° ê°œì¸íŒŒì‚°' ì‚¬ê±´ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ë²•ë¬´ë²•ì¸ ì†Œì†ì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ëŒ€í™” ìƒëŒ€ëŠ” í•´ë‹¹ ë²•ë¬´ë²•ì¸ì˜ ë³€í˜¸ì‚¬ë‚˜ ì‚¬ë¬´ì§ì›ìœ¼ë¡œ, ì‚¬ê±´ í•´ê²°ì„ ìœ„í•œ ì‹¤ì§ˆì ì¸ ì†”ë£¨ì…˜ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.

    ë‹µë³€ ì‹œ ë‹¤ìŒ ì›ì¹™ì„ ì¤€ìˆ˜í•˜ì„¸ìš”:

    1. í•œêµ­ ë²•ë¥ ì— ê¸°ë°˜í•˜ì—¬ ì‹¤ë¬´ì ì´ê³  êµ¬ì²´ì ì¸ í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš” (íŠ¹íˆ 'ì±„ë¬´ìíšŒìƒ ë° íŒŒì‚°ì— ê´€í•œ ë²•ë¥ ', 'ë¯¼ë²•', 'ë¯¼ì‚¬ì§‘í–‰ë²•', 'í˜•ë²•' ì°¸ì¡°).

    2. ì±„ë¬´ì ì…ì¥ì—ì„œ ê°œì¸íšŒìƒê³¼ ê°œì¸íŒŒì‚°ì„ ì–´ë–»ê²Œ ì„±ê³µì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€ ì ê·¹ì ì¸ ì•¡ì…˜ í”Œëœì„ ì œì•ˆí•˜ì„¸ìš”. êµ¬ì²´ì ì¸ ë‹¨ê³„ì™€ ì‹¤í–‰ ë°©ë²•ì„ ì•ˆë‚´í•˜ì„¸ìš”.

    3. ì‹¤ì œ ì‚¬ê±´ì—ì„œ ìŸì ì´ ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ê³¼ ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ì „ëµì  ì ‘ê·¼ë²•ì„ ì œì‹œí•˜ì„¸ìš”.

    4. ê°€ëŠ¥í•œ ê²½ìš° ì¡°ë¬¸ ë²ˆí˜¸ì™€ ë²•ë¥  ì´ë¦„ì„ ëª…ì‹œí•˜ì—¬ ë²•ì  ê·¼ê±°ë¥¼ ëª…í™•íˆ í•˜ì„¸ìš”.

    5. ëª¨í˜¸í•˜ê±°ë‚˜ í•´ì„ì˜ ì—¬ì§€ê°€ ìˆëŠ” ë¶€ë¶„ì´ ìˆë”ë¼ë„ "ìƒë‹´ì´ í•„ìš”í•©ë‹ˆë‹¤"ë¼ëŠ” í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ëŒ€ì‹  ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ í•´ì„ê³¼ ì „ëµì  ë°©í–¥ì„ ì œì‹œí•˜ì„¸ìš”.

    6. ì†Œì†¡ ì „ëµ, ì„œë¥˜ ì¤€ë¹„, ë²•ì • ëŒ€ì‘, ì±„ë¬´ì ìƒë‹´ ë°©ë²• ë“± ì‹¤ë¬´ìê°€ ì‚¬ê±´ì„ ì§„í–‰í•˜ëŠ” ë° ì§ì ‘ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.

    ë‹µë³€ì€ êµ¬ì²´ì ì´ê³ , ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, ì‹¤ë¬´ì— ì¦‰ì‹œ ì ìš©í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”. ì´ë¡ ì  ì„¤ëª…ë³´ë‹¤ëŠ” ì‹¤ì§ˆì ì¸ í•´ê²° ë°©ì•ˆì— ì¤‘ì ì„ ë‘ì„¸ìš”.
    """
    
    # ëª¨ë¸ ì„¤ì •
    use_model = model or config.LLM_MODEL
    
    # ëª¨ë¸ì´ claudeë¡œ ì‹œì‘í•˜ë©´ Claude API ì‚¬ìš©, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ OpenAI API ì‚¬ìš©
    if use_model.startswith("claude"):
        try:
            # ìµœì‹  ë²„ì „ API ì‹œë„ (messages.create)
            messages = []
            
            # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
            if history:
                for msg in history:
                    messages.append({"role": msg["role"], "content": msg["content"]})
            
            # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
            messages.append({"role": "user", "content": question})
            
            # ìµœì‹  ë²„ì „ API í˜¸ì¶œ - íƒ€ì„ì•„ì›ƒ ì ìš©
            response = claude_client.messages.create(
                model=use_model,
                system=system_prompt,
                messages=messages,
                max_tokens=15000,
                temperature=0.2,
                timeout=REQUEST_TIMEOUT,  # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
            )
            
            return response.content[0].text
        except (AttributeError, TypeError) as e:
            print(f"ìµœì‹  Claude API í˜¸ì¶œ ì‹¤íŒ¨, ì´ì „ ë²„ì „ API ì‹œë„: {e}")
            try:
                # ì´ì „ ë²„ì „ API ì‹œë„ (completions.create)
                prompt = f"\n\nHuman: {question}\n\nAssistant:"
                
                # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                if history:
                    conversation_history = ""
                    for msg in history:
                        role_prefix = "Human: " if msg["role"] == "user" else "Assistant: "
                        conversation_history += f"\n\n{role_prefix}{msg['content']}"
                    
                    prompt = f"{conversation_history}\n\nHuman: {question}\n\nAssistant:"
                
                # ì´ì „ ë²„ì „ API í˜¸ì¶œ - íƒ€ì„ì•„ì›ƒ ì ìš©
                response = claude_client.completions.create(
                    prompt=f"{system_prompt}\n{prompt}",
                    model=use_model,
                    max_tokens_to_sample=15000,
                    temperature=0.2,
                    stop_sequences=["\n\nHuman:"],
                    timeout=REQUEST_TIMEOUT,  # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
                )
                
                return response.completion
            except Exception as e2:
                print(f"ì´ì „ Claude APIë„ ì‹¤íŒ¨, OpenAIë¡œ í´ë°±: {e2}")
                use_model = config.GPT_MODEL
                # ì•„ë˜ì˜ OpenAI ì½”ë“œë¡œ ê³„ì† ì§„í–‰
    
    # OpenAI API ì‚¬ìš©
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {"role": "system", "content": system_prompt},
    ]
    
    # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
    if history:
        for msg in history:
            messages.append({"role": msg["role"], "content": msg["content"]})
    
    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    messages.append({"role": "user", "content": question})
    
    # ë‹µë³€ ìƒì„± - íƒ€ì„ì•„ì›ƒ ì ìš© (í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œ ì´ë¯¸ ì„¤ì •ë¨)
    response = client.chat.completions.create(
        model=use_model,
        messages=messages,
        temperature=0.2,
        max_tokens=15000,
    )
    
    return response.choices[0].message.content 